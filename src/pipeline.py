from __future__ import annotations

from pathlib import Path
from typing import List

from .lexer import tokenize
from . import lalr
from .utils import (
    StageResult,
    UserError,
    ensure_input_file,
    ensure_output_dir,
    write_csv_with_header,
    write_text_file,
    write_tokens_csv,
    write_action_goto_csv,
)

SUPPORTED_STAGES = ["lexer", "table", "parse", "ir", "opt", "codegen", "all"]


def run_stage(stage: str, input_path: str) -> StageResult:
    """Dispatch a single stage and return basic metadata about the outputs."""
    normalized = stage.lower()
    if normalized not in SUPPORTED_STAGES:
        raise UserError(f"Error: unsupported stage '{stage}'")

    source_path = ensure_input_file(input_path)
    out_dir = ensure_output_dir(source_path)
    generated: List[Path] = []

    if normalized == "lexer":
        generated.append(_emit_tokens(source_path, out_dir))
    elif normalized == "table":
        generated.append(_emit_action_goto(out_dir))
    elif normalized == "parse":
        generated.append(_emit_parse_trace(out_dir))
    elif normalized == "ir":
        generated.append(_emit_ir(out_dir))
    elif normalized == "opt":
        generated.extend(_emit_opt(out_dir))
    elif normalized == "codegen":
        generated.append(_emit_target(out_dir))
    elif normalized == "all":
        generated.extend(_run_all(source_path, out_dir))

    return StageResult(stage=normalized, output_dir=out_dir, generated=generated)


def _emit_tokens(source_path: Path, out_dir: Path) -> Path:
    path = out_dir / "tokens.csv"
    tokens = tokenize(source_path)
    write_tokens_csv(path, tokens)
    return path


def _emit_action_goto(out_dir: Path) -> Path:
    path = out_dir / "action_goto.csv"
    _, terminals, nonterminals, action, goto_table = lalr.generate_tables()
    write_action_goto_csv(path, terminals, nonterminals, action, goto_table)
    return path


def _emit_parse_trace(out_dir: Path) -> Path:
    path = out_dir / "parse_trace.txt"
    content = (
        "state_stack\tsymbol_stack\tremaining_input\taction\n"
        "0\t<stub>\t<stub>\tshift (parser not implemented)\n"
    )
    write_text_file(path, content)
    return path


def _emit_ir(out_dir: Path) -> Path:
    path = out_dir / "ir.quad"
    content = (
        "# IR quad list (stub)\n"
        "# format: index: (op, arg1, arg2, result)\n"
    )
    write_text_file(path, content)
    return path


def _emit_opt(out_dir: Path) -> List[Path]:
    ir_opt = out_dir / "ir_opt.quad"
    report = out_dir / "opt_report.txt"

    write_text_file(
        ir_opt,
        "# Optimized IR quad list (stub)\n# format: index: (op, arg1, arg2, result)\n",
    )

    report_lines = [
        "Pass pipeline: constant-folding -> dead-code-elimination (stub)",
        "Basic blocks:",
        "  B0: start -> <none>",
        "Changes:",
        "  removed indices: []",
        "  replacements: []",
        "Stats:",
        "  before: 0 instructions",
        "  after: 0 instructions",
    ]
    write_text_file(report, "\n".join(report_lines) + "\n")
    return [ir_opt, report]


def _emit_target(out_dir: Path) -> Path:
    path = out_dir / "target.asm"
    content = (
        "; Target assembly (stub)\n"
        "; This file is generated by the codegen stage placeholder.\n"
    )
    write_text_file(path, content)
    return path


def _run_all(source_path: Path, out_dir: Path) -> List[Path]:
    generated: List[Path] = []
    generated.append(_emit_tokens(source_path, out_dir))
    generated.append(_emit_action_goto(out_dir))
    generated.append(_emit_parse_trace(out_dir))
    generated.append(_emit_ir(out_dir))
    generated.extend(_emit_opt(out_dir))
    generated.append(_emit_target(out_dir))
    return generated
